#!/usr/bin/env bash


# MLE small, no dropout
python main-seq2seq.py --dataset nmt --dataroot data/iwlst14_de-en/iwlst14_de-en_train_test.train.pt --rollin gt --objective mle --obj_normalization cell-global --revert_input_sequence --memory_size 256 --memory_size_encoder 256 --rnn_depth 1 --bidirectional --attention --attn_type sum-tanh --dropout 0.0 --input_feed 0 --optim_method adam --learning_rate 1e-3 --anneal_learning_rate --lr_quantity_to_monitor dataset_specific_val --lr_quantity_mode max --lr_quantity_epsilon 1e-4 --lr_reduce_factor 0.5 --lr_min_value 1e-5  --lr_patience 5000 --lr_cooldown 10000 --lr_quantity_smoothness 1000 --max_iter 0 --print_iter 1 --eval_iter 100 --save_iter 500 --eval_size 1500 --checkpoint_file results/iwlst14/runs_20170725/mle_gt_adam_lr1e-3_depth1_memory256_v2_attn-sum-tanh_dropout0_normGlobal_annealLr/checkpoint_iter_16500.pth
# BLEU - 22.65

# MLE small with dropout
python main-seq2seq.py --dataset nmt --dataroot data/iwlst14_de-en/iwlst14_de-en_train_test.train.pt --rollin gt --objective mle --obj_normalization cell-global --revert_input_sequence --memory_size 256 --memory_size_encoder 256 --rnn_depth 1 --bidirectional --attention --attn_type sum-tanh --dropout 0.3 --input_feed 0 --optim_method adam --learning_rate 1e-3 --anneal_learning_rate --lr_quantity_to_monitor dataset_specific_val --lr_quantity_mode max --lr_quantity_epsilon 1e-4 --lr_reduce_factor 0.5 --lr_min_value 1e-5  --lr_patience 5000 --lr_cooldown 10000 --lr_quantity_smoothness 1000 --max_iter 0 --print_iter 1 --eval_iter 100 --save_iter 500 --eval_size 1500 --checkpoint_file results/iwlst14/runs_20170725/mle_gt_adam_lr1e-3_depth1_memory256_v2_attn-sum-tanh_dropout3e-1_normGlobal_annealLr/checkpoint_iter_26500.pth
# BLEU - 23.98

# Tl small no dropout
python main-seq2seq.py --dataset nmt --dataroot data/iwlst14_de-en/iwlst14_de-en_train_test.train.pt --rollin gt --rollout mixed --objective target-learning --loss bleu-smoothed --obj_normalization cell-global --revert_input_sequence --memory_size 256 --memory_size_encoder 256 --rnn_depth 1 --bidirectional --attention --attn_type sum-tanh --dropout 0.0 --input_feed 0 --num_cells_to_rollout 20 --sample_labels_uniform 0 --sample_labels_policy_topk 15 --sample_labels_neighbors 5  --optim_method adam --learning_rate 1e-3 --anneal_learning_rate --lr_quantity_to_monitor dataset_specific_val --lr_quantity_mode max --lr_quantity_epsilon 1e-4 --lr_reduce_factor 0.5 --lr_min_value 1e-5  --lr_patience 5000 --lr_cooldown 10000 --lr_quantity_smoothness 1000 --max_iter 0 --print_iter 1 --eval_iter 100 --save_iter 500 --eval_size 1500  --rollout_batch_size 256 --checkpoint_file results/iwlst14/runs_20170725/target-learning_bleu-smoothed_gt_mixed_adam_lr1e-3_depth1_memory256_v2_attn-sum-tanh_dropout0_inputFeed_normGlobal_annealLr/checkpoint_iter_29000.pth
# BLEU - 24.50

# Tl small with dropout
python main-seq2seq.py --dataset nmt --dataroot data/iwlst14_de-en/iwlst14_de-en_train_test.train.pt --rollin gt --rollout mixed --objective target-learning --loss bleu-smoothed --obj_normalization cell-global --revert_input_sequence --memory_size 256 --memory_size_encoder 256 --rnn_depth 1 --bidirectional --attention --attn_type sum-tanh --dropout 0.3 --input_feed 0 --num_cells_to_rollout 20 --sample_labels_uniform 0 --sample_labels_policy_topk 15 --sample_labels_neighbors 5  --optim_method adam --learning_rate 1e-3 --anneal_learning_rate --lr_quantity_to_monitor dataset_specific_val --lr_quantity_mode max --lr_quantity_epsilon 1e-4 --lr_reduce_factor 0.5 --lr_min_value 1e-5  --lr_patience 5000 --lr_cooldown 10000 --lr_quantity_smoothness 1000 --max_iter 0 --print_iter 1 --eval_iter 100 --save_iter 500 --eval_size 1500  --rollout_batch_size 256 --checkpoint_file results/iwlst14/runs_20170725/target-learning_bleu-smoothed_gt_mixed_adam_lr1e-3_depth1_memory256_v2_attn-sum-tanh_dropout3e-1_normGlobal_annealLr/checkpoint_iter_33500.pth
# BLEU - 24.34

# MLE large
python main-seq2seq.py --dataset nmt --dataroot data/iwlst14_de-en/iwlst14_de-en_train_test.train.pt --rollin gt --objective mle --obj_normalization cell-global --revert_input_sequence --memory_size 256 --memory_size_encoder 256 --rnn_depth 2 --bidirectional --attention --attn_type sum-tanh --dropout 0.3 --input_feed 1 --num_cells_to_rollout 100 --optim_method adam --learning_rate 1e-3 --anneal_learning_rate --lr_quantity_to_monitor dataset_specific_val --lr_quantity_mode max --lr_quantity_epsilon 1e-4 --lr_reduce_factor 0.5 --lr_min_value 1e-5  --lr_patience 5000 --lr_cooldown 10000 --lr_quantity_smoothness 1000  --max_iter 0 --print_iter 1 --eval_iter 100 --save_iter 500 --eval_size 1500 --checkpoint_file results/iwlst14/runs_20170725/mle_gt_adam_lr1e-3_depth2_memory256_v2_attn-sum-tanh2_dropout3e-1_inputFeed_normGlobal_annealLr/checkpoint_iter_200000.pth
# BLEU - 27.57

# TL large
python main-seq2seq.py --dataset nmt --dataroot data/iwlst14_de-en/iwlst14_de-en_train_test.train.pt --rollin gt --rollout mixed --objective target-learning --loss bleu-smoothed --obj_normalization cell-global --revert_input_sequence --memory_size 256 --memory_size_encoder 256 --rnn_depth 2 --bidirectional --attention --attn_type sum-tanh --dropout 0.3 --input_feed 1 --num_cells_to_rollout 20 --sample_labels_uniform 0 --sample_labels_policy_topk 15 --sample_labels_neighbors 5  --optim_method adam --learning_rate 1e-3 --anneal_learning_rate --lr_quantity_to_monitor dataset_specific_val --lr_quantity_mode max --lr_quantity_epsilon 1e-4 --lr_reduce_factor 0.5 --lr_min_value 1e-5  --lr_patience 5000 --lr_cooldown 10000 --lr_quantity_smoothness 1000 --max_iter 0 --print_iter 1 --eval_iter 100 --save_iter 500 --eval_size 1500 --rollout_batch_size 256 --checkpoint_file results/iwlst14/runs_20170725/target-learning_bleu-smoothed_gt_mixed_adam_lr1e-3_depth2_memory256_v2_attn-sum-tanh_dropout3e-1_inputFeed_normGlobal_annealLr_/checkpoint_iter_200000.pth 
# BLEU - 28.09


# TL large, older model; !do not forget to add one more tanh back to the attention
python main-seq2seq.py --dataset nmt --dataroot data/iwlst14_de-en/iwlst14_de-en_train_test.train.pt --rollin gt --rollout mixed --objective target-learning --loss bleu-smoothed --obj_normalization cell-global --revert_input_sequence --memory_size 256 --rnn_depth 2 --bidirectional --attention --attn_type sum-tanh --dropout 0.3 --input_feed 1 --num_cells_to_rollout 20 --sample_labels_uniform 0 --sample_labels_policy_topk 50 --sample_labels_neighbors 50  --optim_method adam --learning_rate 1e-3 --anneal_learning_rate --lr_quantity_to_monitor dataset_specific_val --lr_quantity_mode max --lr_quantity_epsilon 1e-4 --lr_reduce_factor 0.5 --lr_min_value 1e-5  --lr_patience 5000 --lr_cooldown 10000 --max_iter 0 --print_iter 1 --eval_iter 200 --save_iter 2000 --eval_size 1500 --checkpoint_file results/iwlst14/runs_20170623/target-learning_bleu-smoothed_gt_mixed_adam_lr1e-3_depth2_memory256_attn-sum-tanh_dropout3e-1_inputFeed_normGlobal_annealLr_numLabels100/checkpoint_iter_150000.pth
# BLEU - 28.01

# corresponding MLE: !do not forget to add one more tanh back to the attention
python main-seq2seq.py --dataset nmt --dataroot data/iwlst14_de-en/iwlst14_de-en_train_test.train.pt --rollin gt --objective mle --obj_normalization cell-global --revert_input_sequence --memory_size 256 --rnn_depth 2 --bidirectional --attention --attn_type sum-tanh --dropout 0.3 --input_feed 1 --num_cells_to_rollout 100 --sample_labels_uniform 10 --sample_labels_policy_topk 5 --sample_labels_neighbors 2  --optim_method adam --learning_rate 1e-3 --anneal_learning_rate --lr_quantity_to_monitor dataset_specific_val --lr_quantity_mode max --lr_quantity_epsilon 1e-4 --lr_reduce_factor 0.5 --lr_min_value 1e-5  --lr_patience 5000 --lr_cooldown 10000 --max_iter 0 --print_iter 1 --eval_iter 200 --save_iter 1000 --eval_size 1500 --checkpoint_file results/iwlst14/runs_20170623/mle_gt_adam_lr1e-3_depth2_memory256_attn-sum-tanh_dropout3e-1_inputFeed_normGlobal_annealLr/checkpoint_iter_200000.pth
# BLEU - 27.43

